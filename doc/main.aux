\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\AC@reset@newl@bel
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{mitchell97}
\citation{alphago}
\citation{go}
\citation{supervised-learning}
\citation{mnist}
\citation{unsupervised-learning}
\citation{semi-supervised-learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to machine learning}{2}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Learning types}{2}{section.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}Supervised learning}{2}{subsection.1.1.1}}
\newlabel{sec:supervised-learning}{{1.1.1}{2}{Supervised learning}{subsection.1.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Unsupervised learning}{2}{subsection.1.1.2}}
\citation{reinforcement-learning}
\citation{reinforcement-learning-silver15}
\citation{class-and-regr}
\citation{mnist}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Semi-supervised learning}{3}{subsection.1.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.4}Reinforcement learning}{3}{subsection.1.1.4}}
\AC@undonewlabel{acro:RL}
\newlabel{acro:RL}{{1.1.4}{3}{Reinforcement learning}{section*.2}{}}
\acronymused{RL}
\acronymused{RL}
\acronymused{RL}
\acronymused{ML}
\acronymused{RL}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Classification and regression}{3}{section.1.2}}
\citation{ms11}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Decision Tree}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces \textbf  {Binary decision tree. (a)} Trees are hierarchical structures consisting of nodes and edges. In contrast to graphs, trees do not have loops. \textbf  {(b)} Example decision tree that detects primitive geometries in an image. }}{4}{figure.2.1}}
\newlabel{fig:dtree}{{2.1}{4}{\textbf {Binary decision tree. (a)} Trees are hierarchical structures consisting of nodes and edges. In contrast to graphs, trees do not have loops. \textbf {(b)} Example decision tree that detects primitive geometries in an image}{figure.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Building a decision tree}{4}{section.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{Example: toy production line}{4}{section*.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \textbf  {Toy production line.} The toys produced by the company in the example.}}{5}{figure.2.2}}
\newlabel{fig:collection}{{2.2}{5}{\textbf {Toy production line.} The toys produced by the company in the example}{figure.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Information theory}{5}{subsection.2.1.1}}
\citation{log-complexity}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces \textbf  {Sample training set.} A possible (part of a) training set for the decision tree in the toy company example. $x_1$ to $x_4$ are discrete attributes while $x_5$ is continuous.}}{6}{table.2.1}}
\newlabel{tab:training-sample}{{2.1}{6}{\textbf {Sample training set.} A possible (part of a) training set for the decision tree in the toy company example. $x_1$ to $x_4$ are discrete attributes while $x_5$ is continuous}{table.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Entropy}{6}{section*.4}}
\acronymused{ML}
\@writefile{toc}{\contentsline {subsubsection}{Gini impurity}{6}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{Information gain}{6}{section*.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}ID3}{6}{subsection.2.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \textbf  {Distributions and their entropies.} Three different distributions (uniform, normal and binary on a set with 10000 samples) of the \textit  {accessory} attribute in the toy company example. The bottom right chart shows the entropy for each distribution. The less distributed the set is, the lower is the entropy. }}{7}{figure.2.3}}
\newlabel{fig:entropy}{{2.3}{7}{\textbf {Distributions and their entropies.} Three different distributions (uniform, normal and binary on a set with 10000 samples) of the \textit {accessory} attribute in the toy company example. The bottom right chart shows the entropy for each distribution. The less distributed the set is, the lower is the entropy}{figure.2.3}{}}
\bibstyle{ieeetr}
\bibdata{sources}
\newacro{ML}[\AC@hyperlink{ML}{ML}]{machine learning}
\newacro{RL}[\AC@hyperlink{RL}{RL}]{reinforcement learning}
\bibcite{mitchell97}{1}
\bibcite{alphago}{2}
\bibcite{go}{3}
\bibcite{supervised-learning}{4}
\bibcite{mnist}{5}
\bibcite{unsupervised-learning}{6}
\bibcite{semi-supervised-learning}{7}
\bibcite{reinforcement-learning}{8}
\bibcite{reinforcement-learning-silver15}{9}
\bibcite{class-and-regr}{10}
\bibcite{ms11}{11}
\bibcite{log-complexity}{12}
